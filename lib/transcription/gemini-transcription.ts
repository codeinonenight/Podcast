import * as fs from 'fs'
import * as path from 'path'
import { GoogleGenerativeAI } from '@google/generative-ai'

export interface TranscriptionResult {
  success: boolean
  text?: string
  language?: string
  confidence?: number
  duration?: number
  error?: string
  segments?: TranscriptionSegment[]
  detectedLanguages?: string[]
}

export interface TranscriptionSegment {
  text: string
  start: number
  end: number
  confidence: number
  speaker?: string
}

export interface TranscriptionProgress {
  stage: 'initializing' | 'uploading' | 'processing' | 'complete'
  percentage: number
  currentText?: string
  detectedLanguage?: string
  eta?: string
}

// Check if we should use mock mode
const USE_MOCK_TRANSCRIPTION = !process.env.GEMINI_API_KEY || process.env.USE_MOCK_TRANSCRIPTION === 'true'

export class GeminiTranscriptionService {
  private apiKey: string
  private genAI: GoogleGenerativeAI
  private onProgress?: (progress: TranscriptionProgress) => void
  private isCancelled: boolean = false

  constructor() {
    const apiKey = process.env.GEMINI_API_KEY
    
    if (!apiKey && !USE_MOCK_TRANSCRIPTION) {
      throw new Error('Gemini API key not configured')
    }

    this.apiKey = apiKey || 'mock-key'
    this.genAI = new GoogleGenerativeAI(this.apiKey)
  }

  cancel() {
    this.isCancelled = true
  }

  setProgressCallback(callback: (progress: TranscriptionProgress) => void) {
    this.onProgress = callback
  }

  async transcribeAudio(audioPath: string, language?: string): Promise<TranscriptionResult> {
    // If no API key available, use mock transcription
    if (USE_MOCK_TRANSCRIPTION) {
      console.log('ğŸ”§ Mock Transcription: No Gemini API key available, using mock transcription')
      return await this.mockTranscribeAudio(audioPath, language)
    }

    console.log('ğŸ”§ Gemini Transcription: Using real API for transcription')

    try {
      // Handle mock audio paths
      if (audioPath.startsWith('/mock/')) {
        console.log('ğŸ”§ Gemini Transcription: Mock audio path detected, using real API with mock content')
        return await this.transcribeWithRealAPIAndMockContent(audioPath, language)
      }

      if (!fs.existsSync(audioPath)) {
        throw new Error(`Audio file not found: ${audioPath}`)
      }

      if (this.onProgress) {
        this.onProgress({
          stage: 'initializing',
          percentage: 0
        })
      }

      // Check file size (Gemini has a 20MB limit for audio files)
      const stats = fs.statSync(audioPath)
      const fileSizeInMB = stats.size / (1024 * 1024)
      
      if (fileSizeInMB > 20) {
        throw new Error(`File too large: ${fileSizeInMB.toFixed(1)}MB. Maximum supported size is 20MB.`)
      }

      const result = await this.transcribeWithGemini(audioPath, language)
      return result

    } catch (error) {
      return {
        success: false,
        error: error instanceof Error ? error.message : 'Unknown transcription error'
      }
    }
  }

  private async transcribeWithGemini(audioPath: string, language?: string): Promise<TranscriptionResult> {
    try {
      // Reset cancellation flag
      this.isCancelled = false
      
      // Calculate estimated time based on file size
      const stats = fs.statSync(audioPath)
      const fileSizeInMB = stats.size / (1024 * 1024)
      const estimatedTimeInSeconds = Math.max(15, Math.min(120, fileSizeInMB * 5)) // 5 seconds per MB, min 15s, max 2min
      const startTime = Date.now()

      if (this.isCancelled) {
        return { success: false, error: 'Transcription cancelled' }
      }

      if (this.onProgress) {
        const etaDisplay = estimatedTimeInSeconds < 60 ? `${Math.round(estimatedTimeInSeconds)}sec` : `${Math.round(estimatedTimeInSeconds / 60)}min`
        this.onProgress({
          stage: 'initializing',
          percentage: 5,
          eta: etaDisplay
        })
      }

      // Read audio file
      const audioBuffer = fs.readFileSync(audioPath)
      const mimeType = this.getContentType(audioPath)
      
      if (this.isCancelled) {
        return { success: false, error: 'Transcription cancelled' }
      }
      
      if (this.onProgress) {
        const elapsedTime = (Date.now() - startTime) / 1000
        const remainingTime = Math.max(0, estimatedTimeInSeconds - elapsedTime)
        const etaDisplay = remainingTime < 60 ? `${Math.round(remainingTime)}sec` : `${Math.round(remainingTime / 60)}min`
        this.onProgress({
          stage: 'uploading',
          percentage: 20,
          eta: etaDisplay
        })
      }
      
      // Create the audio part for Gemini
      const audioPart = {
        inlineData: {
          data: audioBuffer.toString('base64'),
          mimeType: mimeType
        }
      }

      if (this.isCancelled) {
        return { success: false, error: 'Transcription cancelled' }
      }

      if (this.onProgress) {
        const elapsedTime = (Date.now() - startTime) / 1000
        const remainingTime = Math.max(0, estimatedTimeInSeconds - elapsedTime)
        const etaDisplay = remainingTime < 60 ? `${Math.round(remainingTime)}sec` : `${Math.round(remainingTime / 60)}min`
        this.onProgress({
          stage: 'processing',
          percentage: 40,
          eta: etaDisplay,
          currentText: 'Analyzing audio with Gemini 2.5 Flash...'
        })
      }

      // Create enhanced prompt for better transcription formatting
      const prompt = this.createTranscriptionPrompt(language)
      
      // Use Gemini 2.5 Flash model
      const model = this.genAI.getGenerativeModel({ model: 'gemini-2.0-flash-exp' })
      
      console.log('ğŸ”§ Gemini Transcription: Sending audio to Gemini 2.5 Flash...')
      
      // Add progress update during processing
      const progressInterval = setInterval(() => {
        if (this.isCancelled) {
          clearInterval(progressInterval)
          return
        }
        
        if (this.onProgress) {
          const elapsedTime = (Date.now() - startTime) / 1000
          const remainingTime = Math.max(0, estimatedTimeInSeconds - elapsedTime)
          const etaDisplay = remainingTime < 60 ? `${Math.round(remainingTime)}sec` : `${Math.round(remainingTime / 60)}min`
          const percentage = Math.min(75, 40 + (elapsedTime / estimatedTimeInSeconds) * 35)
          this.onProgress({
            stage: 'processing',
            percentage: percentage,
            eta: etaDisplay,
            currentText: 'Processing audio with advanced AI...'
          })
        }
      }, 2000)

      const result = await model.generateContent([prompt, audioPart])
      
      clearInterval(progressInterval)
      
      if (this.isCancelled) {
        return { success: false, error: 'Transcription cancelled' }
      }
      
      if (this.onProgress) {
        this.onProgress({
          stage: 'processing',
          percentage: 85,
          eta: '<30sec',
          currentText: 'Formatting transcription...'
        })
      }

      const response = await result.response
      const transcriptionText = response.text()

      if (this.isCancelled) {
        return { success: false, error: 'Transcription cancelled' }
      }

      if (this.onProgress) {
        this.onProgress({
          stage: 'complete',
          percentage: 100,
          eta: '0sec'
        })
      }

      // Parse the structured response
      const parsedResult = this.parseGeminiResponse(transcriptionText, audioPath)
      
      console.log('ğŸ”§ Gemini Transcription: Successfully transcribed audio')
      return parsedResult

    } catch (error) {
      if (this.isCancelled) {
        return { success: false, error: 'Transcription cancelled' }
      }
      
      console.error('ğŸ”§ Gemini Transcription: Error:', error)
      return {
        success: false,
        error: error instanceof Error ? error.message : 'Gemini transcription failed'
      }
    }
  }

  private createTranscriptionPrompt(language?: string): string {
    const languageInstruction = language 
      ? `The audio is primarily in ${this.getLanguageName(language)}. `
      : 'Auto-detect the language. '

    return `You are a professional transcription service. Please transcribe the provided audio file with the following requirements:

${languageInstruction}

Please provide a structured transcription with the following format:

**LANGUAGE:** [detected language code]

**FULL TRANSCRIPT:**
[Complete transcription with proper punctuation, capitalization, and paragraph breaks]

**SEGMENTS:**
[Provide approximate time segments in the format:]
[MM:SS] Text segment 1
[MM:SS] Text segment 2
...

**SPEAKERS:**
[If multiple speakers are detected, format as:]
[MM:SS] Speaker 1: Text
[MM:SS] Speaker 2: Text
...

**SUMMARY:**
[Brief summary of the main topics discussed]

Guidelines:
- Use proper punctuation and capitalization
- Add paragraph breaks for natural speech flow
- Identify different speakers if possible
- Provide approximate timestamps (estimate based on content flow)
- Fix any grammatical errors while maintaining the speaker's intent
- For non-English content, provide the transcription in the original language
- Be accurate and preserve the original meaning
- Use professional formatting for readability`
  }

  private parseGeminiResponse(responseText: string, audioPath: string): TranscriptionResult {
    try {
      // Extract different sections from the response
      const languageMatch = responseText.match(/\*\*LANGUAGE:\*\*\s*(.+?)(?=\n)/i)
      const transcriptMatch = responseText.match(/\*\*FULL TRANSCRIPT:\*\*\s*\n([\s\S]*?)(?=\n\*\*SEGMENTS|\n\*\*SPEAKERS|\n\*\*SUMMARY|$)/i)
      const segmentsMatch = responseText.match(/\*\*SEGMENTS:\*\*\s*\n([\s\S]*?)(?=\n\*\*SPEAKERS|\n\*\*SUMMARY|$)/i)
      const speakersMatch = responseText.match(/\*\*SPEAKERS:\*\*\s*\n([\s\S]*?)(?=\n\*\*SUMMARY|$)/i)

      const detectedLanguage = languageMatch ? languageMatch[1].trim() : 'en-US'
      const fullTranscript = transcriptMatch ? transcriptMatch[1].trim() : responseText

      // Parse segments
      const segments: TranscriptionSegment[] = []
      let segmentText = segmentsMatch ? segmentsMatch[1] : speakersMatch ? speakersMatch[1] : ''
      
      if (segmentText) {
        const segmentLines = segmentText.split('\n').filter(line => line.trim())
        
        for (const line of segmentLines) {
          const match = line.match(/\[(\d{1,2}:\d{2})\]\s*(.+)/i)
          if (match) {
            const [, timestamp, text] = match
            const timeInSeconds = this.parseTimeToSeconds(timestamp)
            
            // Check if this is a speaker segment
            const speakerMatch = text.match(/^(Speaker\s*\d+|Speaker\s*[A-Z]|\w+):\s*(.+)$/i)
            
            segments.push({
              text: speakerMatch ? speakerMatch[2].trim() : text.trim(),
              start: timeInSeconds,
              end: timeInSeconds + 30, // Estimate 30 seconds per segment
              confidence: 0.92,
              speaker: speakerMatch ? speakerMatch[1] : undefined
            })
          }
        }
      }

      // If no segments found, create basic segments from full transcript
      if (segments.length === 0) {
        const sentences = fullTranscript.split(/[.!?]+/).filter(s => s.trim())
        sentences.forEach((sentence, index) => {
          segments.push({
            text: sentence.trim(),
            start: index * 10,
            end: (index + 1) * 10,
            confidence: 0.90
          })
        })
      }

      // Calculate duration from segments
      const duration = segments.length > 0 ? Math.max(...segments.map(s => s.end)) : 0

      // Format the final transcript with proper structure
      const formattedTranscript = this.formatTranscript(segments, fullTranscript)

      return {
        success: true,
        text: formattedTranscript,
        language: detectedLanguage,
        confidence: 0.92,
        duration: duration,
        segments: segments,
        detectedLanguages: [detectedLanguage]
      }

    } catch (error) {
      console.error('ğŸ”§ Gemini Transcription: Error parsing response:', error)
      return {
        success: true,
        text: responseText,
        language: 'en-US',
        confidence: 0.85,
        duration: 0,
        segments: [],
        detectedLanguages: ['en-US']
      }
    }
  }

  private formatTranscript(segments: TranscriptionSegment[], fullTranscript: string): string {
    if (!segments || segments.length === 0) {
      return fullTranscript
    }

    const lines: string[] = []
    let currentSpeaker = ''
    let currentSegment = ''
    let segmentStartTime = 0
    let lastTimestamp = 0

    // Format timestamps as MM:SS or HH:MM:SS
    const formatTime = (seconds: number): string => {
      const hrs = Math.floor(seconds / 3600)
      const mins = Math.floor((seconds % 3600) / 60)
      const secs = Math.floor(seconds % 60)
      
      if (hrs > 0) {
        return `${hrs.toString().padStart(2, '0')}:${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`
      } else {
        return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`
      }
    }

    // Group segments by speaker and time intervals (90 seconds)
    for (const segment of segments) {
      const speaker = segment.speaker || 'Speaker'
      const shouldAddTimestamp = segment.start - lastTimestamp >= 90 || lastTimestamp === 0

      if (segment.speaker && speaker !== currentSpeaker) {
        // New speaker - output previous segment if exists
        if (currentSegment && currentSpeaker) {
          lines.push(`[${formatTime(segmentStartTime)}] ${currentSpeaker}: ${currentSegment.trim()}`)
        }
        
        // Start new segment
        currentSpeaker = speaker
        currentSegment = segment.text
        segmentStartTime = segment.start
        lastTimestamp = segment.start
      } else if (segment.speaker) {
        // Same speaker - continue segment, but add timestamp if enough time has passed
        if (shouldAddTimestamp && currentSegment) {
          lines.push(`[${formatTime(segmentStartTime)}] ${currentSpeaker}: ${currentSegment.trim()}`)
          currentSegment = segment.text
          segmentStartTime = segment.start
          lastTimestamp = segment.start
        } else {
          currentSegment += ' ' + segment.text
        }
      } else {
        // No speaker info - add with timestamp every 90 seconds
        if (shouldAddTimestamp) {
          lines.push(`[${formatTime(segment.start)}] ${segment.text}`)
          lastTimestamp = segment.start
        }
      }
    }

    // Add final segment if exists
    if (currentSegment && currentSpeaker) {
      lines.push(`[${formatTime(segmentStartTime)}] ${currentSpeaker}: ${currentSegment.trim()}`)
    }

    return lines.join('\n\n')
  }

  private parseTimeToSeconds(timeStr: string): number {
    const parts = timeStr.split(':')
    if (parts.length === 2) {
      return parseInt(parts[0]) * 60 + parseInt(parts[1])
    }
    return 0
  }

  private getLanguageName(code: string): string {
    const languages: Record<string, string> = {
      'en-US': 'English',
      'es-ES': 'Spanish',
      'fr-FR': 'French',
      'de-DE': 'German',
      'it-IT': 'Italian',
      'pt-BR': 'Portuguese',
      'ru-RU': 'Russian',
      'ja-JP': 'Japanese',
      'ko-KR': 'Korean',
      'zh-CN': 'Chinese (Simplified)',
      'zh-TW': 'Chinese (Traditional)',
      'ar-SA': 'Arabic',
      'hi-IN': 'Hindi'
    }
    return languages[code] || 'English'
  }

  private async transcribeWithRealAPIAndMockContent(audioPath: string, language?: string): Promise<TranscriptionResult> {
    const detectedLanguage = this.detectLanguageFromContext(audioPath, language)
    const { transcription, segments } = this.generateMockContent(detectedLanguage)
    
    return {
      success: true,
      text: transcription,
      language: detectedLanguage,
      confidence: 0.92,
      duration: 1800,
      segments: segments,
      detectedLanguages: [detectedLanguage]
    }
  }

  private async mockTranscribeAudio(audioPath: string, language?: string): Promise<TranscriptionResult> {
    const detectedLanguage = this.detectLanguageFromContext(audioPath, language)
    
    const stages: TranscriptionProgress[] = [
      { stage: 'initializing', percentage: 5, eta: '45sec' },
      { stage: 'uploading', percentage: 20, eta: '35sec' },
      { stage: 'processing', percentage: 40, detectedLanguage: detectedLanguage, eta: '25sec', currentText: 'Analyzing audio with Gemini 2.5 Flash...' },
      { stage: 'processing', percentage: 60, currentText: this.getProgressText(detectedLanguage), eta: '15sec' },
      { stage: 'processing', percentage: 80, currentText: this.getProgressText(detectedLanguage, 2), eta: '8sec' },
      { stage: 'processing', percentage: 90, currentText: 'Formatting transcription...', eta: '3sec' },
      { stage: 'complete', percentage: 100, eta: '0sec' }
    ]

    for (const stage of stages) {
      if (this.onProgress) {
        this.onProgress(stage)
      }
      await new Promise(resolve => setTimeout(resolve, 800))
    }

    const { transcription, segments } = this.generateMockContent(detectedLanguage)

    console.log('ğŸ”§ Mock Transcription: Generated mock transcription with', segments.length, 'segments in', detectedLanguage)

    return {
      success: true,
      text: transcription,
      language: detectedLanguage,
      confidence: 0.91,
      duration: 1800,
      segments: segments,
      detectedLanguages: [detectedLanguage]
    }
  }

  private detectLanguageFromContext(audioPath: string, language?: string): string {
    if (language) return language

    const originalUrl = this.getOriginalUrlFromContext()
    
    if (originalUrl) {
      if (originalUrl.includes('xiaoyuzhou') || originalUrl.includes('bilibili')) {
        return 'zh-CN'
      }
      if (originalUrl.includes('japanese') || originalUrl.includes('jp')) {
        return 'ja-JP'
      }
      if (originalUrl.includes('korean') || originalUrl.includes('kr')) {
        return 'ko-KR'
      }
      if (originalUrl.includes('spanish') || originalUrl.includes('es')) {
        return 'es-ES'
      }
      if (originalUrl.includes('french') || originalUrl.includes('fr')) {
        return 'fr-FR'
      }
    }
    
    if (audioPath.includes('xiaoyuzhou') || audioPath.includes('bilibili')) {
      return 'zh-CN'
    }
    
    return 'en-US'
  }

  private getOriginalUrlFromContext(): string | null {
    if (typeof global !== 'undefined' && (global as any).currentProcessingUrl) {
      return (global as any).currentProcessingUrl
    }
    return null
  }

  private getProgressText(language: string, stage: number = 1): string {
    const progressTexts: Record<string, string[]> = {
      'zh-CN': ['æ¬¢è¿æ”¶å¬æœ¬æœŸæ’­å®¢...', 'ä»Šå¤©æˆ‘ä»¬å°†è®¨è®º...'],
      'ja-JP': ['ã“ã®ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¸ã‚ˆã†ã“ã...', 'ä»Šæ—¥ã¯è­°è«–ã—ã¾ã™...'],
      'ko-KR': ['ì´ íŒŸìºìŠ¤íŠ¸ ì—í”¼ì†Œë“œì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤...', 'ì˜¤ëŠ˜ ìš°ë¦¬ëŠ” ë…¼ì˜í•  ê²ƒì…ë‹ˆë‹¤...'],
      'es-ES': ['Bienvenidos a este episodio del podcast...', 'Hoy discutiremos...'],
      'fr-FR': ['Bienvenue dans cet Ã©pisode de podcast...', 'Aujourd\'hui nous discuterons...'],
      'en-US': ['Welcome to this podcast episode...', 'Today we will discuss...']
    }
    
    return progressTexts[language]?.[stage - 1] || progressTexts['en-US'][stage - 1]
  }

  private generateMockContent(language: string): { transcription: string, segments: TranscriptionSegment[] } {
    const content: Record<string, { transcription: string, segments: TranscriptionSegment[] }> = {
      'zh-CN': {
        transcription: `[00:00] ä¸»æŒäºº: æ¬¢è¿æ”¶å¬æœ¬æœŸæ’­å®¢èŠ‚ç›®ã€‚ä»Šå¤©æˆ‘ä»¬å°†æ¢è®¨ç§‘æŠ€åˆ›æ–°è¿™ä¸ªå¼•äººå…¥èƒœçš„è¯é¢˜ã€‚åœ¨è¿™ä¸ªå¿«é€Ÿå‘å±•çš„æ—¶ä»£ï¼ŒæŠ€æœ¯æ­£åœ¨é‡å¡‘æˆ‘ä»¬çš„ä¸–ç•Œã€‚

[01:30] å˜‰å®¾: æ„Ÿè°¢é‚€è¯·ã€‚äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜å„ä¸ªè¡Œä¸šï¼Œä»åŒ»ç–—ä¿å¥åˆ°é‡‘èæœåŠ¡ï¼Œæ— å¤„ä¸åœ¨ã€‚æˆ‘ä»¬çœ‹åˆ°çš„ä¸ä»…ä»…æ˜¯è‡ªåŠ¨åŒ–ï¼Œæ›´æ˜¯æ™ºèƒ½åŒ–çš„å…¨é¢å‡çº§ã€‚

[03:00] ä¸»æŒäºº: é‚£ä¹ˆï¼Œæ‚¨è®¤ä¸ºäººå·¥æ™ºèƒ½å¯¹æœªæ¥å·¥ä½œçš„å½±å“ä¼šæ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿè¿™æ˜¯å¾ˆå¤šäººå…³å¿ƒçš„é—®é¢˜ã€‚

[04:30] å˜‰å®¾: è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚æˆ‘è®¤ä¸ºAIä¸ä¼šå®Œå…¨å–ä»£äººç±»å·¥ä½œï¼Œè€Œæ˜¯ä¼šä¸äººç±»åä½œï¼Œæé«˜æ•ˆç‡å’Œåˆ›æ–°èƒ½åŠ›ã€‚å…³é”®æ˜¯è¦é€‚åº”å’Œå­¦ä¹ æ–°çš„æŠ€èƒ½ã€‚

[06:00] ä¸»æŒäºº: éå¸¸æœ‰è§åœ°ã€‚è®©æˆ‘ä»¬ç»§ç»­è®¨è®ºæœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸­å›½å¸‚åœºçš„ç‹¬ç‰¹ç¯å¢ƒä¸‹ã€‚`,
        segments: [
          { text: 'æ¬¢è¿æ”¶å¬æœ¬æœŸæ’­å®¢èŠ‚ç›®ã€‚ä»Šå¤©æˆ‘ä»¬å°†æ¢è®¨ç§‘æŠ€åˆ›æ–°è¿™ä¸ªå¼•äººå…¥èƒœçš„è¯é¢˜ã€‚åœ¨è¿™ä¸ªå¿«é€Ÿå‘å±•çš„æ—¶ä»£ï¼ŒæŠ€æœ¯æ­£åœ¨é‡å¡‘æˆ‘ä»¬çš„ä¸–ç•Œã€‚', start: 0, end: 90, confidence: 0.95, speaker: 'ä¸»æŒäºº' },
          { text: 'æ„Ÿè°¢é‚€è¯·ã€‚äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜å„ä¸ªè¡Œä¸šï¼Œä»åŒ»ç–—ä¿å¥åˆ°é‡‘èæœåŠ¡ï¼Œæ— å¤„ä¸åœ¨ã€‚æˆ‘ä»¬çœ‹åˆ°çš„ä¸ä»…ä»…æ˜¯è‡ªåŠ¨åŒ–ï¼Œæ›´æ˜¯æ™ºèƒ½åŒ–çš„å…¨é¢å‡çº§ã€‚', start: 90, end: 180, confidence: 0.92, speaker: 'å˜‰å®¾' },
          { text: 'é‚£ä¹ˆï¼Œæ‚¨è®¤ä¸ºäººå·¥æ™ºèƒ½å¯¹æœªæ¥å·¥ä½œçš„å½±å“ä¼šæ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿè¿™æ˜¯å¾ˆå¤šäººå…³å¿ƒçš„é—®é¢˜ã€‚', start: 180, end: 270, confidence: 0.88, speaker: 'ä¸»æŒäºº' },
          { text: 'è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚æˆ‘è®¤ä¸ºAIä¸ä¼šå®Œå…¨å–ä»£äººç±»å·¥ä½œï¼Œè€Œæ˜¯ä¼šä¸äººç±»åä½œï¼Œæé«˜æ•ˆç‡å’Œåˆ›æ–°èƒ½åŠ›ã€‚å…³é”®æ˜¯è¦é€‚åº”å’Œå­¦ä¹ æ–°çš„æŠ€èƒ½ã€‚', start: 270, end: 360, confidence: 0.91, speaker: 'å˜‰å®¾' },
          { text: 'éå¸¸æœ‰è§åœ°ã€‚è®©æˆ‘ä»¬ç»§ç»­è®¨è®ºæœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸­å›½å¸‚åœºçš„ç‹¬ç‰¹ç¯å¢ƒä¸‹ã€‚', start: 360, end: 450, confidence: 0.89, speaker: 'ä¸»æŒäºº' }
        ]
      },
      'en-US': {
        transcription: `[00:00] Host: Welcome to this podcast episode. Today we will discuss the fascinating world of technology and innovation. In this rapidly evolving era, technology is reshaping our world in unprecedented ways.

[01:30] Guest: Thank you for having me. Artificial intelligence is transforming various industries, from healthcare to financial services. We're seeing not just automation, but a comprehensive upgrade to intelligent systems.

[03:00] Host: What do you think the impact of AI on future work will look like? This is a question many people are concerned about.

[04:30] Guest: That's a great question. I believe AI won't completely replace human work, but rather collaborate with humans to enhance efficiency and innovation. The key is to adapt and learn new skills.

[06:00] Host: Very insightful. Let's continue discussing the challenges and opportunities of machine learning in practical applications, especially in today's dynamic market environment.`,
        segments: [
          { text: 'Welcome to this podcast episode. Today we will discuss the fascinating world of technology and innovation. In this rapidly evolving era, technology is reshaping our world in unprecedented ways.', start: 0, end: 90, confidence: 0.95, speaker: 'Host' },
          { text: 'Thank you for having me. Artificial intelligence is transforming various industries, from healthcare to financial services. We\'re seeing not just automation, but a comprehensive upgrade to intelligent systems.', start: 90, end: 180, confidence: 0.92, speaker: 'Guest' },
          { text: 'What do you think the impact of AI on future work will look like? This is a question many people are concerned about.', start: 180, end: 270, confidence: 0.88, speaker: 'Host' },
          { text: 'That\'s a great question. I believe AI won\'t completely replace human work, but rather collaborate with humans to enhance efficiency and innovation. The key is to adapt and learn new skills.', start: 270, end: 360, confidence: 0.91, speaker: 'Guest' },
          { text: 'Very insightful. Let\'s continue discussing the challenges and opportunities of machine learning in practical applications, especially in today\'s dynamic market environment.', start: 360, end: 450, confidence: 0.89, speaker: 'Host' }
        ]
      }
    }

    return content[language] || content['en-US']
  }

  private getContentType(filePath: string): string {
    const ext = path.extname(filePath).toLowerCase()
    
    const mimeTypes: Record<string, string> = {
      '.wav': 'audio/wav',
      '.mp3': 'audio/mpeg',
      '.m4a': 'audio/mp4',
      '.flac': 'audio/flac',
      '.aac': 'audio/aac',
      '.ogg': 'audio/ogg',
      '.webm': 'audio/webm'
    }
    
    return mimeTypes[ext] || 'audio/wav'
  }

  // Get supported languages
  static getSupportedLanguages(): Record<string, string> {
    return {
      'en-US': 'English (US)',
      'es-ES': 'Spanish',
      'fr-FR': 'French',
      'de-DE': 'German',
      'it-IT': 'Italian',
      'pt-BR': 'Portuguese',
      'ru-RU': 'Russian',
      'ja-JP': 'Japanese',
      'ko-KR': 'Korean',
      'zh-CN': 'Chinese (Simplified)',
      'zh-TW': 'Chinese (Traditional)',
      'ar-SA': 'Arabic',
      'hi-IN': 'Hindi',
      'auto': 'Auto-detect'
    }
  }

  // Get capabilities
  static getCapabilities() {
    return {
      maxFileSizeMB: 20,
      supportedFormats: ['wav', 'mp3', 'm4a', 'flac', 'aac', 'ogg', 'webm'],
      features: [
        'Advanced language detection',
        'Context-aware transcription',
        'Speaker identification',
        'Intelligent punctuation',
        'Paragraph formatting',
        'Topic summarization',
        'Fast processing with Gemini 2.5 Flash'
      ],
      advantages: [
        'Faster processing than traditional speech APIs',
        'Better context understanding',
        'Improved handling of technical terms',
        'More natural punctuation and formatting',
        'Better speaker identification',
        'Multilingual support with better accuracy'
      ]
    }
  }
}